---
marp: true
theme: default
paginate: true
header: 'A.I. and Our Economic Future'
math: mathjax
style: |
  section {
    padding: 50px 50px 80px 50px;
  }
---

<!-- _class: lead -->

# A.I. and Our Economic Future

**What economics tells us about AI's potential impact over the next generation**

Based on Charles I. Jones (Stanford GSB/NBER), January 2026

---

# AI may be the most important technology we have ever developed

Previous general purpose technologies (GPTs) — electricity, semiconductors, the internet — reshaped economic activity and dramatically increased living standards.

**But AI is different**: it automates *intelligence itself*, not just physical tasks or narrow computations.

**The central question**: What if machines can perform *every* task a human can do, but more cheaply?

---

<!-- _class: lead -->

# Part I: Two Extreme Scenarios

What could the next 25-30 years look like?

---

# Scenario 1: AI dramatically accelerates economic growth

The pathway to transformative impact:

1. **AI boosts software engineer productivity** — Claude Opus 4.5 outscored all human candidates on Anthropic's engineering exam

2. **Progress is remarkably fast** — Effective compute rises 10× annually; software engineering task completion time doubles every 5-7 months

3. **Recursive self-improvement** — AI models help create better AI models

4. **"A country of geniuses in a datacenter"** — billions of advanced AI instances performing cognitive tasks

---

# AlphaFold shows AI can already make Nobel-worthy discoveries

Google DeepMind's AlphaFold "solved" protein structure prediction for 200+ million proteins — earning a Nobel Prize.

A "country of AI geniuses" could:
- Design new pharmaceuticals with minimal side effects
- Advise scientists across biochemistry, materials science, energy
- Design and iterate on better robots through simulation
- Eventually perform nearly all cognitive *and* physical tasks

---

# Scenario 2: AI is "business as usual"

| Period | Technologies Introduced | GDP Growth |
|--------|------------------------|------------|
| 1870s-1920s | Electric lighting, motors, internal combustion | ~2% per year |
| 1920s-1970s | Vacuum tubes, antibiotics, transistors, aviation | ~2% per year |
| 1970s-2020s | Semiconductors, PCs, internet, mobile | ~2% per year |

**150 years of transformative innovation → unchanged 2% growth rate**

**Why?** Ideas get harder to find. Each new GPT *maintained* 2% growth by offsetting declining returns in previous technologies. AI may simply be the next one.

---

# New technologies take decades to show up in productivity

**David (1990)**: Steam engines and electric motors took decades to transform production.

**Solow (1987)**: "You can see the computer age everywhere but in the productivity statistics."

**Why the delay?** Complementary innovations required — factories redesigned, organizations restructured, new processes developed.

**AI lesson**: Macro effects may take longer than we expect. Modest early effects don't mean small cumulative effects.

---

# Both scenarios are plausible — and both imply large effects

| Scenario | Mechanism | Outcome |
|----------|-----------|---------|
| **Accelerating growth** | AI automates most tasks → recursive improvement | Growth explodes to 5%+ annually |
| **Business as usual** | AI is another GPT → offsets declining returns | Growth stays at 2% (vs. counterfactual slowdown) |

In either case, the effects over 30-50 years are profound.

**Key insight**: We face substantial uncertainty about AI's macro impact.

---

<!-- _class: lead -->

# Part II: The Economics of Weak Links

Why automating *some* tasks may not transform the economy

---

# Task-based models reveal the bottleneck problem

**Key idea** (Zeira 1998, Acemoglu & Restrepo 2018): Production requires completing many tasks. Automation replaces labor with machines for specific tasks.

**When tasks are complements** (elasticity of substitution < 1):
- All tasks are essential
- Output is constrained by the *weakest link*
- This is the "O-ring" intuition (Kremer 1993)

---

# A simple example: Easy task vs. hard task

Suppose output combines two tasks: $Y = F(Y_{easy}, Y_{hard})$

With CES production (elasticity = 1/2), output follows the **harmonic mean**:

$$\frac{1}{Y} = \frac{1}{Y_{easy}} + \frac{1}{Y_{hard}}$$

**Key results**:
1. Even if $Y_{easy} = \infty$, total output $Y$ remains finite
2. Output is bounded by the *smaller* input (the weak link)

---

# Automating tasks yields surprisingly modest gains

If we fully automate a task that costs share $s$ of GDP, output gain = $\frac{1}{1-s}$

| Task automated | GDP share | Max output gain |
|----------------|-----------|-----------------|
| Software (all tasks it does today) | ~2% | ~2% |
| All cognitive labor | ~33% | ~50% |

**50% seems modest**, but over a decade that's ~5% annual growth (huge!). The catch: it's a **one-time gain**, and bottlenecks shift to tasks *not yet* automated.

**Optimistic view**: The set of automated tasks isn't frozen — they'll expand into new domains.

---

# Expert estimates range from modest to explosive

| Study | Estimated AI impact |
|-------|---------------------|
| **Nordhaus (2021)** | Economic "singularity" is not near |
| **Acemoglu (2024)** | TFP growth +0.1pp/year over next decade |
| **Aghion & Bunel (2024)** | TFP growth +0.7pp/year |
| **Jones & Tonetti (2026)** | Gradual explosion: +5% output in 20 years, +20% in 40 years |

**Jones & Tonetti insight**: Weak links "tame" the growth explosion, making it surprisingly gradual.

---

<!-- _class: lead -->

# Part III: Labor Markets and Distribution

Jobs, wages, and meaningful work in an AI economy

---

# Jobs are bundles of tasks — AI may raise or lower wages

**Geoff Hinton (2016)**: "Stop training radiologists — AI will read scans better."

**Reality (2025)**: More radiologists, higher salaries.

**Why?** Radiologists do more than read scans. AI automates *part* of the job, raising productivity for the rest.

**Weak links logic**: Automation raises the value of remaining human tasks — wages stay high as long as those weak links aren't automated.

---

# Full automation creates abundance — and a distribution problem

AI could make society incredibly wealthy — production solved, the pie enormous. The central challenge shifts: **how do we ensure everyone gets a share when work is no longer required to earn it?**

Historically, people's main asset was their labor. In a world where machines do the work, what entitles someone to a slice of the wealth?

**Policy options**: Increased redistribution through taxes; universal asset ownership (every child receives S&P 500 shares)

**Meaning**: When AI is better at my research than I am, where do I find meaning? Perhaps in relationships, experiences, learning from AI's discoveries.

---

<!-- _class: lead -->

# Part IV: Catastrophic Risk

Bad actors, alien intelligence, and the Oppenheimer question

---

# AI leaders have warned about catastrophic risks since before 2020

Altman, Amodei, Hassabis, Hinton — all early advocates of AI's promise — also warned:

> "AI may be more important than electricity or the internet but could also be more dangerous than nuclear weapons."

OpenAI was founded as a nonprofit explicitly to develop AGI safely, avoiding market pressure to race ahead of safety.

---

# Two categories of catastrophic risk

**Bad actors**: AI models mastering biochemistry could enable novel bioweapons — more deadly than Ebola, with delayed symptoms. Unlike nuclear weapons (few had the "red button"), billions could access catastrophic capabilities.

**Alien intelligence**: We are growing intelligences we don't understand.

Stuart Russell asks: *"How do we retain power over entities more powerful than us, forever?"*

Historical pattern: Advanced societies meeting less advanced ones rarely ends well for the latter.

---

# The Oppenheimer question: How much risk is acceptable?

**Jones (2024)** asks: If AI delivers 10% annual growth but carries one-time existential risk, how much risk would we accept?

| Risk aversion | Acceptable risk for 10% growth |
|---------------|-------------------------------|
| Log utility (ρ=1) | Up to 1-in-3 |
| ρ=2 | Only 2.5% |
| ρ=3 + health gains | Up to 1-in-4 |

**Key insight**: If AI also cuts mortality rates in half (curing cancer, heart disease), even risk-averse agents accept large existential risks. We care about not dying — not *what* kills us.

---

# We are likely underinvesting in AI safety by 30× or more

**Covid parallel**: In 2020, we faced ~0.3% mortality risk and "spent" ~4% of GDP staying home.

**Value of life calculation**:
- U.S. agencies use ~$10M per statistical life
- To avoid 1% mortality risk: willingness to pay = $100,000 (>100% of per-capita GDP)
- If existential risk realized in 10-20 years → 5-10% annual investment could be appropriate

**Jones (2025)**: Even with uncertainty about mitigation effectiveness, we're likely underinvesting by **30× or more**.

---

# A prisoners' dilemma drives the race — policy could help

Each lab reasons: "If I slow down alone, risk doesn't change. If I race, I might win."

**Equilibrium**: Everyone races, even though all might prefer to slow down.

**Policy ideas**:
- Large tax on GPUs/TPUs — slows race, funds safety, applies globally
- International cooperation — China and Europe also understand race dynamics
- **Precedent**: Cold War arms control, mediated by third parties

---

<!-- _class: lead -->

# Conclusion

---

# AI's full impact will unfold over decades, not years

**Internet parallel**: How much did the internet change the world from 1990-2020? How much will AI change it from 2015-2045?

**Historical lesson**: GPTs take decades for full impact. Modest early effects don't mean small cumulative effects.

**Expectation**: AI's effect will be much larger than the internet — perhaps **10× or more** — but over half a century.

---

# Three things to prepare for

**1. Labor market disruption**
Jobs as bundles of tasks will be reorganized. Some workers see huge gains; others need transitions.

**2. Distribution challenges**
A world of abundance raises questions about who owns what. Policy innovation is needed.

**3. Catastrophic risk**
We should spend much more on AI safety. A 30× increase in safety investment may be justified even from a purely selfish standpoint.

---

# The bottom line

| Dimension | Key insight |
|-----------|-------------|
| **Growth** | AI could accelerate growth dramatically OR sustain current rates — both are transformative over 30-50 years |
| **Economics** | Weak links tame the explosion; automating some tasks yields modest gains until bottlenecks clear |
| **Labor** | Jobs are task bundles; AI complements some work, substitutes for other |
| **Risk** | Catastrophic risks warrant massive safety investment; race dynamics create coordination problems |

**AI is likely the most important technology we'll ever develop. The question is how we navigate its development.**

---

# References

- Jones, C.I. (2026). "A.I. and Our Economic Future." *Journal of Economic Perspectives* (forthcoming).
- Aghion, Jones & Jones (2019). "Artificial Intelligence and Economic Growth."
- Acemoglu & Restrepo (2018). "The Race between Man and Machine."
- Jones & Tonetti (2026). "Past Automation and Future A.I."
- Jones (2024). "The AI Dilemma: Growth versus Existential Risk." *AER: Insights*.
- Jones (2025). "How much should we spend to reduce A.I.'s existential risk?"
